{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c615ed7-6f67-46c8-a532-a185aa1e89b7",
   "metadata": {},
   "source": [
    "# Define region names here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83d40353-08c8-4824-8da9-ab09aaf6086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names = ['Lausanne', 'Ouest lausannois']\n",
    "region_tuple_sql = \"(\" + \",\".join(f\"'{r}'\" for r in region_names) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90423c8d-7af1-4c0f-aef0-c4e7b8a82e39",
   "metadata": {},
   "source": [
    "# Environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a9713d-67b8-46a9-9864-efdb7bdaac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupName='L1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0f187f-a7e5-4da6-9072-8cf28908d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: networkx in /home/valat/.local/lib/python3.10/site-packages (3.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d231011a-221a-40fa-8b06-e9f72bde13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"pandas only supports SQLAlchemy connectable .*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8840e1-44ce-4175-81f9-6455014a12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64 as b64\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "def getUsername():\n",
    "    payload = os.environ.get('EPFL_COM490_TOKEN').split('.')[1]\n",
    "    payload=payload+'=' * (4 - len(payload) % 4)\n",
    "    obj = json.loads(b64.urlsafe_b64decode(payload))\n",
    "    if (time.time() > int(obj.get('exp')) - 3600):\n",
    "        raise Exception('Your credentials have expired, please restart your Jupyter Hub server:'\n",
    "                        'File>Hub Control Panel, Stop My Server, Start My Server.')\n",
    "    time_left = int((obj.get('exp') - time.time())/3600)\n",
    "    return obj.get('sub'), time_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfc5819-5eed-46b8-a207-5a840f8ac03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are: valat\n",
      "credentials validity: 167 hours left.\n",
      "shared namespace is: iceberg.com490_iceberg\n",
      "your namespace is: iceberg.valat\n",
      "your group is: L1\n"
     ]
    }
   ],
   "source": [
    "username, validity_h = getUsername()\n",
    "hadoopFS = os.environ.get('HADOOP_FS')\n",
    "namespace = 'iceberg.' + username\n",
    "sharedNS = 'iceberg.com490_iceberg'\n",
    "\n",
    "if not re.search('[A-Z][0-9]', groupName):\n",
    "    raise Exception('Invalid group name {groupName}')\n",
    "\n",
    "print(f\"you are: {username}\")\n",
    "print(f\"credentials validity: {validity_h} hours left.\")\n",
    "print(f\"shared namespace is: {sharedNS}\")\n",
    "print(f\"your namespace is: {namespace}\")\n",
    "print(f\"your group is: {groupName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4412b77e-6b71-4da0-8d1e-416f4e169b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse URL: https://iccluster028.iccluster.epfl.ch:8443/\n",
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "import trino\n",
    "from contextlib import closing\n",
    "from urllib.parse import urlparse\n",
    "from trino.dbapi import connect\n",
    "from trino.auth import BasicAuthentication, JWTAuthentication\n",
    "\n",
    "trinoAuth = JWTAuthentication(os.environ.get('EPFL_COM490_TOKEN'))\n",
    "trinoUrl  = urlparse(os.environ.get('TRINO_URL'))\n",
    "Query=[]\n",
    "\n",
    "print(f\"Warehouse URL: {trinoUrl.scheme}://{trinoUrl.hostname}:{trinoUrl.port}/\")\n",
    "\n",
    "conn = connect(\n",
    "    host=trinoUrl.hostname,\n",
    "    port=trinoUrl.port,\n",
    "    auth=trinoAuth,\n",
    "    http_scheme=trinoUrl.scheme,\n",
    "    verify=True\n",
    ")\n",
    "\n",
    "print('Connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10afd2e3-8d2e-4a7f-9edc-4eff3a2cf2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sbb_calendar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbb_calendar_dates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sbb_istdaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sbb_routes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sbb_stop_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sbb_stops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sbb_stops_lausanne_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sbb_transfers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sbb_trips</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Table\n",
       "0                        geo\n",
       "1               sbb_calendar\n",
       "2         sbb_calendar_dates\n",
       "3               sbb_istdaten\n",
       "4                 sbb_routes\n",
       "5             sbb_stop_times\n",
       "6                  sbb_stops\n",
       "7  sbb_stops_lausanne_region\n",
       "8              sbb_transfers\n",
       "9                  sbb_trips"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_sql(f\"\"\"SHOW TABLES IN {sharedNS}\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd26cec-1d8f-4a2e-ab13-750e1cea8668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sbb_stop_times_lausanne_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sbb_stops_cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sbb_stops_lausanne_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sbb_stops_to_stops_lausanne_region</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Table\n",
       "0      sbb_stop_times_lausanne_region\n",
       "1                   sbb_stops_cleaned\n",
       "2           sbb_stops_lausanne_region\n",
       "3  sbb_stops_to_stops_lausanne_region"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(f\"\"\"SHOW TABLES IN {namespace}\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa3a9d3-2559-43e3-96fe-98ef7035b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_fetch(queries, conn, batch_size=100, include_header=True):\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "        \n",
    "    with closing(conn.cursor()) as cur:\n",
    "        for query in queries:\n",
    "            cur.execute(query)\n",
    "            # Only attempt to fetch results if the query is a SELECT statement\n",
    "            # or another statement that returns data\n",
    "            if cur.description:  # Check if description exists (not None)\n",
    "                if include_header:\n",
    "                    yield [desc[0] for desc in cur.description]\n",
    "                while (rows := cur.fetchmany(batch_size)):\n",
    "                    for row in rows:\n",
    "                        yield row\n",
    "            else:\n",
    "                # For non-SELECT queries (like CREATE, DROP), just yield a status message\n",
    "                yield [\"Query executed successfully\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4adfc3-472e-4264-9f4e-c1b9279b2f3f",
   "metadata": {},
   "source": [
    "# Create and save in HDFS tables related to selected region(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea4878-e5a6-4dc3-a625-a9b28e4e90bb",
   "metadata": {},
   "source": [
    "### First drop all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3be5d36e-66d6-465d-8d97-3c8fa940a0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Table]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sql_fetch([\n",
    "    f\"\"\"DROP SCHEMA IF EXISTS {namespace} CASCADE\"\"\", # CASCADE will drop all the tables\n",
    "    f\"\"\"CREATE SCHEMA IF NOT EXISTS {namespace}\"\"\",\n",
    "], conn))\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS {namespace}\")\n",
    "query = f\"SHOW TABLES IN {namespace}\"\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb4835-f28d-44e6-a355-08bac6c76a9e",
   "metadata": {},
   "source": [
    "### Delete already saved tables from hdfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616676bb-2204-4609-bd70-8f6ca900f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/com-490/group/L1/sbb_stops_cleaned': No such file or directory\n",
      "rm: `/user/com-490/group/L1/sbb_stops_selected_regions': No such file or directory\n",
      "rm: `/user/com-490/group/L1/sbb_stops_to_stops_selected_regions': No such file or directory\n",
      "rm: `/user/com-490/group/L1/sbb_stop_times_selected_regions': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/com-490/group/L1/sbb_stops_cleaned\n",
    "!hdfs dfs -rm -r /user/com-490/group/L1/sbb_stops_selected_regions\n",
    "!hdfs dfs -rm -r /user/com-490/group/L1/sbb_stops_to_stops_selected_regions\n",
    "!hdfs dfs -rm -r /user/com-490/group/L1/sbb_stop_times_selected_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53de015-4664-4844-b232-ff606c86e541",
   "metadata": {},
   "source": [
    "### sbb_stops_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c010dd1e-7f4f-4b35-bfde-6d2c5c3b309d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trino.dbapi.Cursor at 0x7f2845a8e4a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(f\"DROP TABLE IF EXISTS {namespace}.sbb_stops_cleaned\")\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE TABLE {namespace}.sbb_stops_cleaned\n",
    "WITH (\n",
    "  format   = 'PARQUET',\n",
    "  location = '{hadoopFS}/user/com-490/group/L1/sbb_stops_cleaned/'\n",
    ")\n",
    "AS\n",
    "SELECT \n",
    "    stop_id,\n",
    "    SPLIT(\n",
    "        REGEXP_REPLACE(\n",
    "            REGEXP_REPLACE(stop_id, '^Parent', ''),\n",
    "            'P$', ''\n",
    "        ), ':'\n",
    "    )[1] AS stop_id_cleaned,\n",
    "    stop_name,\n",
    "    stop_lat,\n",
    "    stop_lon,\n",
    "    pub_date\n",
    "FROM iceberg.com490_iceberg.sbb_stops\n",
    "WHERE stop_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a7047-0219-4a2a-9a00-9b4dead90d47",
   "metadata": {},
   "source": [
    "### sbb_stops_region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0343e26-58e0-4af9-a222-b85f0783cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table if exists\n",
    "drop_ctas = f\"\"\"\n",
    "    DROP TABLE IF EXISTS {namespace}.sbb_stops_selected_regions\n",
    "\"\"\"\n",
    "\n",
    "# Create the required table\n",
    "create_ctas = f\"\"\"\n",
    "CREATE TABLE {namespace}.sbb_stops_selected_regions\n",
    "WITH (\n",
    "  format   = 'PARQUET',\n",
    "  location = '{hadoopFS}/user/com-490/group/L1/sbb_stops_selected_regions/'\n",
    ")\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "    stops.stop_id_cleaned,\n",
    "    stops.stop_name,\n",
    "    stops.stop_lat,\n",
    "    stops.stop_lon\n",
    "FROM {namespace}.sbb_stops_cleaned AS stops\n",
    "JOIN {sharedNS}.geo AS g\n",
    "  ON g.name IN {region_tuple_sql}\n",
    "WHERE \n",
    "      stops.pub_date >= DATE '2024-07-01'\n",
    "  AND stops.pub_date <  DATE '2024-07-08'\n",
    "  AND ST_Contains(\n",
    "        ST_GeomFromBinary(g.wkb_geometry),\n",
    "        ST_Point(stops.stop_lon, stops.stop_lat)\n",
    "      )\n",
    "\"\"\"\n",
    "\n",
    "result = list(sql_fetch([drop_ctas, create_ctas], conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0510a3b-9df9-413a-b25f-8b04910887d9",
   "metadata": {},
   "source": [
    "## sbb_stops_to_stops_region\n",
    "Contains stops within 500m of each other, as directed pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53f7ae74-6d73-428b-95c5-ab36d3937a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.7 ms, sys: 10 μs, total: 32.7 ms\n",
      "Wall time: 1.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Query executed successfully'], ['rows'], [2920]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "drop_table_sql = f\"\"\"\n",
    "    DROP TABLE IF EXISTS {namespace}.sbb_stops_to_stops_selected_regions\n",
    "\"\"\"\n",
    "\n",
    "create_table_sql = f\"\"\"\n",
    "    CREATE TABLE {namespace}.sbb_stops_to_stops_selected_regions\n",
    "    WITH (\n",
    "      format   = 'PARQUET',\n",
    "      location = '{hadoopFS}/user/com-490/group/L1/sbb_stops_to_stops_selected_regions/'\n",
    "    )\n",
    "    AS\n",
    "    WITH cte_stops AS (\n",
    "        SELECT \n",
    "            stop_id_cleaned,\n",
    "            stop_name,\n",
    "            stop_lat,\n",
    "            stop_lon\n",
    "        FROM {namespace}.sbb_stops_selected_regions\n",
    "    ),\n",
    "    pairs_with_distance AS (\n",
    "        SELECT\n",
    "            A.stop_id_cleaned AS stop_id_a,\n",
    "            B.stop_id_cleaned AS stop_id_b,\n",
    "            (\n",
    "                6371000.0 * 2 * ASIN(\n",
    "                    SQRT(\n",
    "                        SIN(RADIANS((B.stop_lat - A.stop_lat) / 2)) * SIN(RADIANS((B.stop_lat - A.stop_lat) / 2)) +\n",
    "                        COS(RADIANS(A.stop_lat)) * COS(RADIANS(B.stop_lat)) *\n",
    "                        SIN(RADIANS((B.stop_lon - A.stop_lon) / 2)) * SIN(RADIANS((B.stop_lon - A.stop_lon) / 2))\n",
    "                    )\n",
    "                )\n",
    "            ) AS distance\n",
    "        FROM cte_stops A\n",
    "        CROSS JOIN cte_stops B\n",
    "        WHERE A.stop_id_cleaned <> B.stop_id_cleaned\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM pairs_with_distance\n",
    "    WHERE distance <= 500\n",
    "\"\"\"\n",
    "\n",
    "list(sql_fetch([drop_table_sql, create_table_sql], conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25175a-407a-4ab2-af95-e8b010984a3d",
   "metadata": {},
   "source": [
    "## sbb_stop_times_region \n",
    "It contains the stop times and weekdays of trips (trip_id) servicing stops found previously in the selected region(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64539fa5-049b-4402-b5ec-ae0dc2ce4e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.6 ms, sys: 193 μs, total: 56.8 ms\n",
      "Wall time: 7.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Query executed successfully'], ['rows'], [989352]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "drop_table_sql = f\"\"\"\n",
    "    DROP TABLE IF EXISTS {namespace}.sbb_stop_times_selected_regions\n",
    "\"\"\"\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE TABLE {namespace}.sbb_stop_times_selected_regions\n",
    "    WITH (\n",
    "      format   = 'PARQUET',\n",
    "      location = '{hadoopFS}/user/com-490/group/L1/sbb_stop_times_selected_regions/'\n",
    "    )\n",
    "    AS\n",
    "    SELECT \n",
    "        st.trip_id,\n",
    "        st.stop_id,\n",
    "        st.departure_time,\n",
    "        st.arrival_time,\n",
    "        c.monday,\n",
    "        c.tuesday,\n",
    "        c.wednesday,\n",
    "        c.thursday,\n",
    "        c.friday,\n",
    "        c.saturday,\n",
    "        c.sunday\n",
    "    FROM {sharedNS}.sbb_stop_times st\n",
    "    JOIN {sharedNS}.sbb_trips t\n",
    "        ON st.trip_id = t.trip_id\n",
    "        AND st.pub_date = t.pub_date\n",
    "    JOIN {sharedNS}.sbb_calendar c \n",
    "        ON t.service_id = c.service_id \n",
    "        AND t.pub_date = c.pub_date\n",
    "    JOIN {namespace}.sbb_stops_selected_regions slr \n",
    "        ON st.stop_id = slr.stop_id_cleaned\n",
    "    WHERE st.pub_date = (\n",
    "        SELECT MAX(pub_date) \n",
    "        FROM {sharedNS}.sbb_calendar \n",
    "        WHERE pub_date <= DATE '2024-07-07'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "list(sql_fetch([drop_table_sql, query], conn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
